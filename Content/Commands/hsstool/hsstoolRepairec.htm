<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd" MadCap:lastBlockDepth="6" MadCap:lastHeight="3453" MadCap:lastWidth="819">
    <head>
    </head>
    <body>
        <h1>hsstool repairec</h1>
        <p class="TopicTag" MadCap:conditions="General.Online">[Command]</p>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Introduction</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>Use this <MadCap:xref href="hsstoolOverview.htm" target="_popup">hsstool </MadCap:xref> command to check whether a physical node has all of the erasure coded S3 object fragments that it is supposed to have (based on the node’s assigned tokens); and to replace any object fragments that are missing, outdated, or corrupted. Replacement of missing fragments is implemented by decoding erasure coded S3 objects, re-encoding them, and then re-writing the missing fragment(s).</p>
                <p>The HyperStore system automatically uses a combination of <a href="../../Major Features/Automatic Data Repair/DataRepairOverview.htm" target="_popup">proactive repair and scheduled auto-repair</a> to keep the erasure coded data on each node complete and current. However, there are also operational circumstances when you should manually initiate <em>repairec </em>on a specific node (if you use erasure coding in your cluster). The operations during which you should use <em>repairec </em>are:</p>
                <ul>
                    <li>
                        <MadCap:xref href="../../Operations/AddNode.htm">Add a Node</MadCap:xref>
                    </li>
                    <li>
                        <MadCap:xref href="../../Operations/ReplaceNode.htm">Replace a Node</MadCap:xref>
                    </li>
                    <li>
                        <MadCap:xref href="../../Operations/ReplaceDisk.htm">Replace a Disk</MadCap:xref>
                    </li>
                </ul>
                <p>Please refer to those procedures for step-by-step instructions, including the proper use of <em>repairec </em>within the context of the procedure.</p>
                <p class="Important" MadCap:autonum="&lt;b&gt;IMPORTANT: &lt;/b&gt;">The <em>repairec </em>operation is resource-intensive and may take many hours to complete if there is a large amount of data involved. It is best to run this operation at times of relatively low service usage. It is also preferable to run <em>repairec </em>on only one node at a time. If the need arises, <em>repairec </em>supports an option for aborting an in-progress repair operation. To check on repair progress use <MadCap:xref href="hsstoolOpstatus.htm" target="_popup">hsstool opstatus</MadCap:xref> or check the CMC’s <MadCap:xref href="../../CMC Interface/Analytics/Repair Status/RepairStatusOverview.htm" target="_popup">Repair Status</MadCap:xref> page.</p>
                <p class="Note" MadCap:autonum="&lt;b&gt;Note &#160;&lt;/b&gt;">For system maintenance, the <em>repairec </em>operation is automatically run on each node once each 89 days as part of the scheduled auto-repair feature. The nodes are not all repaired simultaneously; instead, the start times are staggered so that only one node is repaired at a time. The auto-repair interval is configurable in the <MadCap:xref href="../../CMC Interface/Cluster/Configuration Settings/ConfigurationSettingsAutoRepair.htm" target="_popup">Auto-Repair Schedule Settings</MadCap:xref> section of the CMC Configuration Settings page.</p>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Command Format</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>You can run the <em>hsstool repairec</em> command through the CMC UI:</p>
                <p>
                    <img src="../../Resources/Images/cmc/cluster/Advanced_repairec.png" style="max-width: 8.00in;" />
                </p>
                <p>Or on the command line:</p><pre xml:space="preserve">[root]# /opt/cloudian/bin/hsstool -h &lt;host&gt; repairec [-f] [-l &lt;true|false&gt;] [computedigest]
[-stop] [-i] [-d &lt;mount-point&gt;] [-vnode &lt;token&gt;]</pre>
                <h3>Parameters</h3>
                <p>This command supports these parameters:</p>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>-h &lt;host&gt; (called "Target Node" in CMC UI)</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Mandatory) Target host to repair.</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>-f</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Optional) By default, when you perform <em>repairec</em> on a specified node the system scans metadata in Cassandra to determine which objects are supposed to have an erasure coded fragment stored on that node, and then that set of objects is assessed and repaired to ensure that the objects' fragments are all present in the proper locations (on the target node as well as on other nodes).</p>
                        <p>If you use the <em>-f</em> option, then instead of the default method the system performs a repair only on objects for which fragments are found on the target node. Consequently, this method cannot repair objects for which the local fragment is missing. This means that to fully repair the erasure coded data on a given node, you must (if you use the <em>-f</em> option) run <em>repairec</em> on every node in the cluster.</p>
                        <p>The <em>-f </em>option behavior is the way that <em>repairec</em> was implemented for HyperStore prior to version 6.0. You should have no reason to use the <em>-f </em>option unless instructed to do so by Cloudian Support.</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>-l &lt;true|false&gt;</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Optional) &#160;Write to a log file a list of all the objects that were repaired. Defaults to true, so you only need to specify an <em>-l </em>option if you do <b>not</b> want repair object logging (in which case you’d specify <em>-l false</em>).</p>
                        <p>The log is named <em>cloudian-hyperstore-repair.log</em> and is written into the Cloudian HyperStore log directory of the target node. Activity associated with a particular instance of a command run is marked with a unique command number.</p>
                        <p class="Note" MadCap:autonum="&lt;b&gt;Note &#160;&lt;/b&gt;">In the CMC UI, this option is presented simply as an "l" checkbox which is checked by default (meaning logging is enabled by default; you can uncheck it if you don’t want repair object logging).</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>computedigest</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Optional) &#160;When doing the repair, compute a fresh digest for each fragment rather than using cached digests. The re-computed digests of the fragments are compared to the original digests of those fragments (stored alongside the fragment data) and if there is a mismatch the object is decoded from healthy fragments and then re-encoded, and the corrupted fragment is replaced by a new and correct fragment.</p>
                        <p>This way of running <em>repairec</em> is resource-intensive.</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>-stop</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Optional) Use <em>hsstool -h &lt;host&gt; repairec -stop</em> to abort an in-progress repair on the specified node. This stops the repair immediately. You can subsequently use the <MadCap:xref href="hsstoolOpstatus.htm" target="_popup">hsstool opstatus</MadCap:xref> command to confirm that the repair has been stopped (status = TERMINATED) and to see how much repair progress had been made before the stop.</p>
                        <p>The next time <em>repairec </em>is run on the node it will start from scratch and do a full repair. It will not resume from where the stopped repair left off.</p>
                        <p>The -<em>stop </em>option stops a single in-progress repair on a single node. It does <b>not</b> disable the HyperStore scheduled auto-repair feature (it does not prevent future scheduled auto-repairs from launching). For information about <b>disabling</b> auto-repair, and about how stopping a particular in-progress repair impacts the auto-repair queue, see <MadCap:xref href="../../Major Features/Automatic Data Repair/DataRepairDisable.htm">Temporarily Disabling Scheduled Auto-Repairs</MadCap:xref>.</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>-i (called "proactive" in the CMC&#160;UI)</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Optional) &#160;Run a "proactive repair" (utilizing a list of object fragments that have recently failed to be written to the node) rather than a regular repair. Under normal circumstances you should not need to use this option. Whenever a node starts up it checks to see whether it’s necessary to run a proactive repair on itself, and if so the proactive repair is automatically executed. Thereafter the nodes checks again — and if necessary executes proactive repair — on a recurring configurable interval (default is every 60 minutes). So proactive repair runs automatically on each node on an as-needed basis. For more information about the proactive repair feature see <MadCap:xref href="../../Major Features/Automatic Data Repair/DataRepairOverview.htm" target="_popup">Automatic Data Repair Feature Overview</MadCap:xref>.</p>
                        <p class="Note" MadCap:autonum="&lt;b&gt;Note &#160;&lt;/b&gt;">In the CMC UI, this option is labeled as "proactive" rather than "i".</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>-d &lt;mount-point&gt;</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Optional) If you use this option the repair is performed only for object fragments mapped to the vNodes that are assigned to the specified HyperStore data mount point (for example <em>/cloudian1</em>). This option may be useful in circumstances where data is known or suspected to be missing or incorrect on a particular disk.</p>
                        <p>If you want to first see what vNodes are assigned to each of a physical node’s mount points, use the <MadCap:xref href="hsstoolLs.htm" target="_popup">hsstool ls</MadCap:xref> command.</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
                <MadCap:dropDown>
                    <MadCap:dropDownHead class="CloudianChildDropDown">
                        <MadCap:dropDownHotspot>-vnode &lt;token&gt;</MadCap:dropDownHotspot>
                    </MadCap:dropDownHead>
                    <MadCap:dropDownBody>
                        <p>(Optional) If you use this option the repair is performed only for object fragments mapped to the specified vNode (identified by its token such as 18315119863185730105557340630830311535). This option may be useful if during a full node repair (or a disk-specific repair), the operation failed for a particular vNode. In that case you can then use the <em>vnode &lt;token&gt;</em> option to retry repairing just that one vNode.</p>
                    </MadCap:dropDownBody>
                </MadCap:dropDown>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Command/Response Example</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>The example below shows a default run of <em>repairec</em>, using no options. Note that the system assigns the command run a numerical identifier — #31 in this example.</p><pre xml:space="preserve">[root]# /opt/cloudian/bin/hsstool -h cloudian-node1 repairec
Executing repairec. computedigest=false logging=false
Repair EC command #31 completed.</pre>
                <p>For more detail on the operation results run <MadCap:xref href="hsstoolOpstatus.htm" target="_popup">hsstool opstatus</MadCap:xref> on the node. The results of this specific repairec run (identified by command number) will remain available through <em>opstatus</em> until either <em>repairec </em>is run again on the node or the HyperStore Service is restarted on the node.</p>
                <p class="Note" MadCap:autonum="&lt;b&gt;Note &#160;&lt;/b&gt;">Since repair is an important operation, it’s recommended that after running repair on a node you check <MadCap:xref href="hsstoolOpstatus.htm" target="_popup">hsstool opstatus</MadCap:xref> to confirm that the operation succeeded.</p>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
    </body>
</html>