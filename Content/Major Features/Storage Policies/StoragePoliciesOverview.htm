<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd" MadCap:lastBlockDepth="6" MadCap:lastHeight="1877" MadCap:lastWidth="815">
    <head>
    </head>
    <body>
        <h1>Storage Policies Feature Overview</h1>
        <p>Storage policies are ways of protecting data so that it’s durable and highly available to users. The HyperStore system lets you pre-configure one or more storage policies. Users when they create a new storage bucket can then choose which pre-configured storage policy to use to protect data in that bucket. <b>Users cannot create buckets until you have created at least one storage policy.</b></p>
        <p>For each storage policy that you create you can choose from either of two data protection methods:</p>
        <ul>
            <li><b>Replication</b> — With replication, a configurable number of copies of each data object are maintained in the system, and each copy is stored on a different node. For example, with 3X replication 3 copies of each object are stored, with each copy on a different node.</li>
            <li><b>Erasure coding</b> — With erasure coding, each object is encoded into a configurable number (known as the "k" value) of data fragments plus a configurable number (the "m" value) of redundant parity fragments. Each of an object’s "k" plus "m" fragments is unique, and each fragment is stored on a different node. The object can be decoded from any "k" number of fragments. To put it another way: the object remains readable even if "m" number of nodes are unavailable.</li>
        </ul>
        <div class="Indent">
            <p>For example, in a 4+2 erasure coding configuration (4 data fragments plus 2 parity fragments), each object is encoded into a total of 6 unique fragments which are stored on 6 different nodes, and the object can be decoded and read so long as any 4 of those 6 fragments are available.</p>
        </div>
        <p>In general, erasure coding requires less storage overhead (the amount of storage required for data redundancy) and results in somewhat longer request latency than replication. Erasure coding is best suited to large objects that are infrequently accessed.</p>
        <p>Regardless of whether you use replication or erasure coding, if your HyperStore system spans multiple data centers, for each storage policy you can also choose how data is allocated across your data centers — for example, you could have a storage policy that for each S3 object stores 3 replicas of the object in each of your data centers; and a second storage policy that erasure codes objects and stores them in just one particular data center.</p>
        <p>Individual storage policies are not confined to dedicated nodes or disk partitions. Instead, all policies utilize all the resources of your cluster, and data stored in association with a particular policy will tend to be spread fairly evenly across the cluster (with the exception that you can limit a policy to a particular data center as noted above). This helps to ensure that regardless of how many or what types of storage policies you configure, and regardless of how much data is stored in association with particular policies, the physical resources of your cluster — disks, CPU, RAM — will be used in an even manner.</p>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Supported Erasure Coding Configurations</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>HyperStore supports several erasure coding configurations, in terms of "k" value (number of data fragments) and "m" value (number of parity fragments). For example:</p>
                <ul>
                    <li><b>2+1</b> — Each object will be encoded into 2 data fragments plus 1 parity fragment, with each fragment stored on a different node. Objects can be read so long as any 2 of the 3 fragments are available.</li>
                    <li><b>4+2</b> — Each object will be encoded into 4 data fragments plus 2 parity fragments, with each fragment stored on a different node. Objects can be read so long as any 4 of the 6 fragments are available.</li>
                </ul>
                <p>In addition to the options above, the system also supports:</p>
                <ul>
                    <li><b>6+2</b>
                    </li>
                    <li><b>8+2</b>
                    </li>
                    <li><b>9+3</b>
                    </li>
                    <li><b>12+4</b>
                    </li>
                </ul>
                <p>The choice among these supported EC configurations is largely a matter of how many HyperStore nodes you have in the data center. For example, 4+2 EC provides a higher degree of protection and availability than 2+1 EC (since with 4+2 EC, objects can be read/written even if 2 of the involved nodes are unavailable) while delivering the same level of storage efficiency (both 2+1 and 4+2 require 50% storage overhead — the parity fragments as a percentage of data fragments). So 4+2 is preferable to 2+1 if you have at least 6 HyperStore nodes in the data center.</p>
                <p>Likewise, compared to a 4+2 configuration, 6+2 EC provides the same degree of resiliency against nodes being unavailable (objects can be read/written even if 2 of the involved nodes are unavailable), while delivering a higher level of storage efficiency (2/6 = only 33% overhead). The cost, compared to a smaller number of fragments, is a very slight increase in the time that it takes to encode and decode objects.</p>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Data Center Assignment</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>If your HyperStore system is deployed across multiple data centers, for each storage policy that you create you can configure a data center assignment scheme for the policy. This determines which of your data centers to use for storing data, for each storage policy.</p>
                <p>You have the option of creating a storage policy that stores data in some of your data centers and not others — for example, store data in DC1 and DC2 but not in DC3. Note, however, that DC3 may be involved in processing S3 requests associated with buckets that use this policy. There is only one S3 service endpoint per region, and incoming S3 requests may resolve to any DC within the region. If the S3 Service in DC3 receives an S3 PUT request in association with a policy that stores data only in DC1 and DC2, it will transmit the uploaded object on to DC1 and D2 (it will not be stored in DC3). Likewise, if DC3 receives an S3 GET request in association with a policy that stores data only in DC1 and DC2, then DC3’s S3 Service will get the object from DC1 or DC2 and pass it on to the client. If you want more absolute barriers so that for example DC3 never touches DC2’s data and vice-versa, you need to set up your system so those DCs are in different <a href="../Service Regions/ServiceRegionsOverview.htm" target="_popup">service regions</a>.</p>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Consistency Levels</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>Also as part of creating storage policies you can configure consistency levels for reads and writes. Consistency levels determine what requirements must be met within the system — in terms of successful reads or writes of data replicas or erasure coded fragments — before a success response can be returned to an S3 client application that has submitted a GET or PUT request.</p>
                <p>For descriptions of supported consistency levels, see <MadCap:xref href="../../CMC Interface/Cluster/Storage Policies/StoragePoliciesConsistencyLevels.htm">Consistency Levels Definitions</MadCap:xref>.</p>
                <p>For diagrams that illustrate consistency levels in action, see <MadCap:xref href="../../Introduction/System Diagrams/ReplicationWriteAvailability.htm">Replication -- Write Availability</MadCap:xref>.</p>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <p>&#160;</p>
        <p><b>See Also:</b>
        </p>
        <ul>
            <li>
                <MadCap:xref href="StoragePoliciesCreate.htm">Creating and Managing Storage Policies</MadCap:xref>
            </li>
        </ul>
    </body>
</html>