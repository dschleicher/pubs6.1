<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd" MadCap:lastBlockDepth="10" MadCap:lastHeight="4821" MadCap:lastWidth="929">
    <head>
    </head>
    <body>
        <h1>Adding a Node</h1>
        <p>This procedure is for adding a node to your HyperStore cluster and having the new node take over some of the storage load from your existing nodes. </p>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Before You Begin</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>Before you add a node to your HyperStore cluster, take these actions to prepare:</p>
                <ul>
                    <li>Use the CMC's <a href="../Resources/Images/cmc/dashboard/Dashboard.png" target="_popup">Dashboard</a> page to <b>verify that no services are currently down</b> in your cluster -- the dashboard's "Cluster Health" section should show a green circle and status should say "Healthy". If the circle is orange then one or more services are down. In that case, check the CMC's <a href="../Resources/Images/cmc/alerts/Alerts.png" target="_popup">Alerts</a> page for alerts regarding downed services and restart any services that are down.</li>
                </ul>
                <div class="Indent">
                    <MadCap:dropDown>
                        <MadCap:dropDownHead class="CloudianChildDropDown">
                            <MadCap:dropDownHotspot><b>If you have a downed node</b> that cannot be brought back up...</MadCap:dropDownHotspot>
                        </MadCap:dropDownHead>
                        <MadCap:dropDownBody>
                            <p> If you have a downed node that cannot be brought back up, then do the following before adding a new node to the cluster:</p>
                            <ol>
                                <li>In the CMC's <a href="../Resources/Images/cmc/cluster/ClusterInfo.png" target="_popup">Cluster Information</a> page, verify that the Redis Credentials master role and the Redis QoS master role are being filled by a node other than the downed node. These service roles must be running in order to add a new node. (If either of these service roles had been running on the node that's down, they should have automatically failed over to a different node.)</li>
                                <li>Still in the CMC's <a href="../Resources/Images/cmc/cluster/ClusterInfo.png" target="_popup">Cluster Information</a> page, verify that the Puppet Master role is being filled by a node other than the downed node. This service role must be running in order to add a new node. If the Puppet Master role is assigned to the downed node, you must manually re-assign this role to a different existing node before adding a new node to your cluster. For instructions see <MadCap:xref href="Change Node Role/MovePuppetMaster.htm" target="_popup">Move the Puppet Master Primary or Backup Role</MadCap:xref>.</li>
                                <li>On the Puppet Master node, launch the installer:</li>
                            </ol>
                            <div class="Indent"><pre>root# /&lt;installation-staging-directory&gt;/cloudianInstall.sh</pre>
                                <p>From the main menu choose Advanced Configuration Options, then from there apply the "Mark host(s) down for maintenance" task to whichever of your hosts is currently down. Then go back to the main menu, choose Cluster Management, and then push the updated settings out to the cluster.</p>
                            </div>
                        </MadCap:dropDownBody>
                    </MadCap:dropDown>
                </div>
                <ul>
                    <li>If you are using the software-only version of HyperStore (as opposed to a HyperStore Appliance), <b>make sure that the new host meets HyperStore host requirements</b>. If you need a refresher see:<ul style="list-style-type: circle;"><li><MadCap:xref href="../Installation/Host Machine Preparation/Software-Only Users/HardwareRequirements.htm" target="_popup">Hardware Requirements</MadCap:xref></li><li><MadCap:xref href="../Installation/Host Machine Preparation/Software-Only Users/OsRequirements.htm" target="_popup">Operating System Requirements</MadCap:xref></li><li><MadCap:xref href="../Installation/Host Machine Preparation/Software-Only Users/FileSystemRequirements.htm" target="_popup">File System Requirements</MadCap:xref></li></ul></li>
                    <li>Because adding a node to your cluster requires potentially large-scale data transfers within the system, before you add a node you should temporarily <b>shut down the scheduled auto-repair feature</b>, so that there are not auto-repairs running at the same time as the node addition process. For instructions see <MadCap:xref href="../Major Features/Automatic Data Repair/DataRepairDisable.htm" target="_popup">Temporarily Disabling Scheduled Auto-Repairs</MadCap:xref>.</li>
                </ul>
                <p class="Important" MadCap:autonum="&lt;b&gt;IMPORTANT: &lt;/b&gt;">Do not add multiple nodes simultaneously. If you want to add multiple nodes, add one node at a time from start to finish for each node.</p>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Adding the Node to the Cluster</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>To add a new node to a HyperStore cluster:</p>
                <ol>
                    <li>Confirm that the data directory mount points on the new node are the same as are used for the existing nodes in your cluster. If they are, proceed to Step 2. You can also proceed directly to Step 2 if the node you are adding is a HyperStore Appliance.</li>
                </ol>
                <div class="Indent">
                    <MadCap:dropDown>
                        <MadCap:dropDownHead class="CloudianChildDropDown">
                            <MadCap:dropDownHotspot><b>If the new node has different mount points</b> than your existing nodes and  is not a HyperStore Appliance...</MadCap:dropDownHotspot>
                        </MadCap:dropDownHead>
                        <MadCap:dropDownBody>
                            <p>If the new node is not a HyperStore Appliance and the node has a different number of mount points or differently named mount points than the existing nodes in your cluster, you must create a <em>&lt;hostname&gt;_fslist.txt</em> file specifying the new node’s file system mount points and place the file in the installation staging directory on the Puppet master.</p>
                            <p>In the <em>&lt;hostname&gt;_fslist.txt</em> file list the HyperStore data mount points in this format:</p><pre xml:space="preserve">&lt;device-name or UUID&gt; &lt;mount-point&gt;
&lt;device-name or UUID&gt; &lt;mount-point&gt;
etc.</pre>
                            <p>For example:</p><pre xml:space="preserve">/dev/sdc1 /cloudian1
/dev/sdd1 /cloudian2
etc.</pre>
                            <p>Optionally, in this file you can also specify one mount point for metadata stored in Cassandra (must include the string "cassandra" in the mount point path) and one mount point for the Cassandra commit log (must include "cassandra_commit" in the mount point path). If you do not specify these Cassandra mount points in <em>&lt;hostname&gt;_fslist.txt</em>, by default the system automatically puts Cassandra data and commit log directories on the OS disk.</p>
                            <p>Do not use symbolic links when specifying HyperStore data or Cassandra mount points.</p>
                        </MadCap:dropDownBody>
                    </MadCap:dropDown>
                </div>
                <ol MadCap:continue="true">
                    <li>In the CMC page <b>Cluster → Nodes → Advanced</b>, select command type "Add/Uninstall Node" and then select command "Add Node". (Alternatively, in the CMC page <b>Cluster → Data Centers</b>, in the display for the data center and rack to which you want to add a node, click the light green cube icon that has a plus sign on it.) Either of these actions opens the Add Node interface.</li>
                </ol>
                <div class="Indent">
                    <p>
                        <img src="../Resources/Images/operations/AddNode.png" class="ImgCloudian" />
                    </p>
                </div>
                <div class="Indent">
                    <p>In the Add Node interface specify the new node’s hostname, IP address, region, data center, and rack. (Note: If you accessed the Add Node iinterface through the <b>Data Centers</b> page, the data center name and rack name will be pre-populated.) If the new node will use a different dedicated interface for internal cluster traffic than other nodes in your cluster use — for example if the new node uses "eth2" for internal traffic while other nodes in your cluster are using "eth1" for internal traffic — enter the "Internal Network Interface Name" (otherwise you can leave this field empty).</p>
                    <p>To securely connect to the new node as it’s being installed and added to your cluster, use <b>one</b> (not both) of the following methods in the Add Node interface:</p>
                    <MadCap:dropDown>
                        <MadCap:dropDownHead class="CloudianChildDropDown">
                            <MadCap:dropDownHotspot>Installation User’s Password</MadCap:dropDownHotspot>
                        </MadCap:dropDownHead>
                        <MadCap:dropDownBody>
                            <p>To securely connect to the new node via password, enter the root user's password in this field (only the root user can be the HyperStore installation user). </p>
                        </MadCap:dropDownBody>
                    </MadCap:dropDown>
                    <MadCap:dropDown>
                        <MadCap:dropDownHead class="CloudianChildDropDown">
                            <MadCap:dropDownHotspot>Private Key Authentication</MadCap:dropDownHotspot>
                        </MadCap:dropDownHead>
                        <MadCap:dropDownBody>
                            <p>As an alternative to using a password to access the new node, you can select the "Private Key Authentication" checkbox. In this case the install script — which is invoked by the Add Node operation — will use the same private key as was used to install the existing cluster. Distribution of the corresponding public key to the new node depends on how you handled SSH key set-up during installation of the existing HyperStore cluster:</p>
                            <ul>
                                <li>If during installation of the cluster you let the install script generate an SSH key pair for you, then distribution of the public key to the new node will be taken care of automatically by the install script.</li>
                                <li>If during installation of the cluster you used your own existing key pair and you copied both the private and the public key into the installation staging directory, then distribution of the public key to the new node will be taken care of automatically by the install script.</li>
                                <li>If during installation of the cluster you used your own existing key pair and you copied only the private key into the installation directory, while doing the distribution of the public key to the target nodes on your own, then you must also copy the public key to the new node on your own, before executing the Add Node operation.</li>
                            </ul>
                        </MadCap:dropDownBody>
                    </MadCap:dropDown>
                </div>
                <ol MadCap:continue="true">
                    <li>Click <b>Execute</b>. In the confirmation dialog that displays, confirm that you want to execute the operation.</li>
                </ol>
                <div class="Indent">
                    <p>The "Add Node" operation may take some time. When the operation completes, HyperStore services are automatically started on the new node.</p>
                </div>
                <p class="NoteIndent" MadCap:autonum="&lt;b&gt;Note: &lt;/b&gt;">The "Add Node" operation is an atomic operation. If the operation fails at any point, any configuration changes made during the operation are rolled back.</p>
                <ol MadCap:continue="true">
                    <li>In the Info command type group, execute the <em>hsstool status</em> command to confirm that the node’s Cassandra status is "Up" (not "Joining").</li>
                </ol>
                <div class="Indent">
                    <p>
                        <img src="../Resources/Images/operations/Status_add-cloudian-node5.png" class="ImgCloudian" />
                    </p>
                    <p>If instead the Cassandra status is still "Joining", wait briefly and then execute the status check again to confirm an "Up" status before continuing to the next stage of this procedure.</p>
                </div>
                <p class="Note" MadCap:autonum="&lt;b&gt;Note &#160;&lt;/b&gt;">When you add a new node to an existing HyperStore cluster, only the common services (S3 Service, Admin Service, HyperStore Service, Cassandra, CMC, Puppet Agent, and Monitoring Agent)  run on the new node, not any specialized services such as Redis Credentials or Redis QoS.</p>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>Shifting Data Load to the New Node</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>When a node is successfully added to a cluster and its Cassandra status is "Up", the system has automatically assigned the new node its own set of new storage tokens. The next step is to shift some of the data load from your existing nodes to the new node, by triggering a repair operation on the new node (which populates the node with data in accordance with the node's storage tokens.) You can do this either through the CMC or by using <em>hsstool</em> on the command line, whichever you prefer. The instructions that follow are oriented toward the CMC but they also indicate the corresponding command line commands.</p>
                <p>In the CMC page <b>Cluster → Nodes → Advanced</b>, do the following:</p>
                <ol>
                    <li>From the Maintenance command type group, execute <em>hsstool repair</em> on the new node. Use the "nokeyspaces" option (since Cassandra keyspaces are repaired on a new node automatically and do not need to be included in the repair operation that you are running). Do not use the "-pr", "-nomt", "-m", "computedigest", or "proactive" options.</li>
                </ol>
                <div class="Indent">
                    <p>
                        <img src="../Resources/Images/operations/Repair_nokeyspaces_cloudian-node5.png" class="ImgCloudian" />
                    </p>
                    <p>If using the command line rather than the CMC, the command is:</p><pre>[root]# /opt/cloudian/bin/hsstool -h &lt;hostname-of-new-node&gt; repair nokeyspaces</pre>
                    <p>The repair operation populates the new node with the S3 object replica data that it’s supposed to have (based on the vNodes that were automatically assigned to the new node when it was added to the cluster). The repair may take many hours, depending on how much data is involved.</p>
                </div>
                <ol MadCap:continue="true">
                    <li>After letting the repair run for a length of time appropriate to your environment, from the Info command type group execute <em>hsstool opstatus </em>to confirm that the "REPAIR" status is COMPLETED.</li>
                </ol>
                <div class="Indent">
                    <p>
                        <img src="../Resources/Images/operations/Opstatus_cloudian-node5.png" class="ImgCloudian" />
                    </p>
                    <p>If using the command line rather than the CMC, the command is:</p><pre>[root]# /opt/cloudian/bin/hsstool -h &lt;hostname-of-new-node&gt; opstatus</pre>
                    <p>If the REPAIR status is still IN-PROGRESS run <em>hsstool opstatus</em> again later to confirm that the repair completed, before proceeding to the next step.</p>
                    <p class="Note" MadCap:autonum="&lt;b&gt;Note &#160;&lt;/b&gt;">The <em>opstatus</em> results will also show "REPAIRCASSANDRA" as COMPLETED. Because you used the "nokeyspaces" option when running <em>repair</em>, the Cassandra part of the repair completes instantly. It’s the copying of S3 object replicas to the node that takes some time; the status of this operation is indicated in the REPAIR section of the <em>opstatus</em> results.</p>
                </div>
                <ol MadCap:continue="true">
                    <li>If you do not use erasure coding in your system, then you are done with shifting data load to the new node and can proceed to re-enabling auto-repair as described below.</li>
                </ol>
                <div class="Indent">
                    <MadCap:dropDown>
                        <MadCap:dropDownHead class="CloudianChildDropDown">
                            <MadCap:dropDownHotspot><b>If you do use erasure coding</b> in your system...</MadCap:dropDownHotspot>
                        </MadCap:dropDownHead>
                        <MadCap:dropDownBody>
                            <p>If you use erasure coding in your system, from the Maintenance command type group execute <em>hsstool repairec</em> on the new node. Do not use the "f" option.</p>
                            <p>
                                <img src="../Resources/Images/operations/Repairec.png" class="ImgCloudian" />
                            </p>
                            <p>If using the command line rather than the CMC, the command is:</p><pre>[root]# /opt/cloudian/bin/hsstool -h &lt;hostname-of-new-node&gt; repairec</pre>
                            <p>The <em>repairec</em> operation populates the new node with the S3 object erasure coded data that it’s supposed to have (based on its assigned vNodes). Wait for the operation to complete before proceeding to re-enabling auto-repair as described below. You can use <em>opstatus</em> to check the node’s REPAIREC status.</p>
                        </MadCap:dropDownBody>
                    </MadCap:dropDown>
                </div>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
        <MadCap:dropDown>
            <MadCap:dropDownHead class="CloudianParentDropDown">
                <MadCap:dropDownHotspot>After Adding a Node</MadCap:dropDownHotspot>
            </MadCap:dropDownHead>
            <MadCap:dropDownBody>
                <p>After completing the add node procedure:</p>
                <ul>
                    <li>Go to the CMC's <a href="../Resources/Images/cmc/cluster/NodeStatus.png" target="_popup">Node Status</a> page and <b>confirm that the newly added node appears</b> in the "Host" drop-down list, and that you can select the node and view its status information.</li>
                    <li><b>Next</b> go to the CMC's <b>Advanced</b> node management page and from the Maintenance command type group execute <em>hsstool autorepair</em> to <b>re-enable the HyperStore auto-repair feature</b> for the cluster. Submit the command to any of the nodes in your cluster. Leave the "t &lt;type&gt;" option unspecified so that all repair types are enabled.</li>
                </ul>
                <div class="Indent">
                    <p>
                        <img src="../Resources/Images/operations/Autorepair_enable_cloudian-node5.png" class="ImgCloudian" />
                    </p>
                </div>
                <div class="Indent">
                    <p>If using the command line rather than the CMC, the command is:</p>
                </div>
                <div class="Indent"><pre>[root]# /opt/cloudian/bin/hsstool -h &lt;any-host&gt; repairqueue -enable true</pre>
                </div>
                <ul>
                    <li><b>Next you should clean the other nodes</b> to remove data that they are no longer responsible for (since some data storage responsibilities have been shifted to the new node). It’s <b>best to wait for 24 hours</b> after completing repair of the new node before you clean the other nodes. This is because by default the cleaning operations will skip objects that were uploaded within the past 24 hours (and so, if you clean the other nodes right after repairing the new node, the cleanup will leave data that those nodes are no longer responsible for but which was uploaded during the 24 hours preceding the addition of the new node).</li>
                </ul>
                <div class="Indent">
                    <p>After waiting 24 hours since completing repair of the new node, do the following:</p>
                    <ol>
                        <li>In the CMC’s <b>Cluster → Nodes → Advanced</b> page, from the Maintenance command type group execute <em>hsstool cleanup</em> on each node in your cluster except for the new node. Use the "allkeyspaces" option and the "x" option.</li>
                    </ol>
                    <div class="Indent">
                        <p>
                            <img src="../Resources/Images/operations/Cleanup_allkeyspaces.png" class="ImgCloudian" />
                        </p>
                        <p><b>It’s OK for <em>cleanup</em> to be running on multiple nodes in parallel</b>. To do so, you need to initiate the cleanups one node at a time, but you don’t need to wait for cleanup to complete on one node before starting it on another node.</p>
                        <p>The <em>cleanup</em> operation cleans the already existing nodes in your cluster of <b>replica data</b> that they are no longer responsible for.</p>
                        <p>If using the command line rather than the CMC, the command is:</p><pre>[root]# /opt/cloudian/bin/hsstool -h &lt;host&gt; cleanup allkeyspaces -x</pre>
                    </div>
                    <ol MadCap:continue="true">
                        <li>If you do not use erasure coding in your system, then you are done with cleaning your existing nodes.</li>
                    </ol>
                    <div class="Indent">
                        <MadCap:dropDown>
                            <MadCap:dropDownHead class="CloudianChildDropDown">
                                <MadCap:dropDownHotspot><b>If you do use erasure coding</b> in your system</MadCap:dropDownHotspot>
                            </MadCap:dropDownHead>
                            <MadCap:dropDownBody>
                                <p>If you use erasure coding in your system, from the Maintenance command type group execute <em>hsstool cleanupec</em> on each node in your cluster except for the new node. Use the "x" option.</p>
                                <p>
                                    <img src="../Resources/Images/operations/Cleanupec.png" class="ImgCloudian" />
                                </p>
                                <p><b>It’s OK for </b><em style="font-weight: bold;">cleanupec</em><b> to be running on multiple nodes in parallel</b>. To do so, you need to initiate the cleanups one node at a time, but you don’t need to wait for <em>cleanupec</em> to complete on one node before starting it on another node.</p>
                                <p>This operation cleans the already existing nodes in your cluster of erasure coded data that they are no longer responsible for.</p>
                                <p>If using the command line rather than the CMC, the command is:</p><pre>[root]# /opt/cloudian/bin/hsstool -h &lt;host&gt; cleanupec -x</pre>
                            </MadCap:dropDownBody>
                        </MadCap:dropDown>
                    </div>
                </div>
            </MadCap:dropDownBody>
        </MadCap:dropDown>
    </body>
</html>